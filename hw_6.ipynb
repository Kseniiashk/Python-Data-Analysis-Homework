{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0mkKcZ71SSU"
   },
   "source": [
    "В этом задании предлагается выполнять задания с помощью pyspark. Запрещено использовать вставки SQL-кода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjkTH8QMMZL9"
   },
   "source": [
    "## **Задача 1: Анализ транзакций клиентов банка (3б)**\n",
    "\n",
    "Представьте, что у нас есть миллион транзакций клиентов банка. Каждая транзакция имеет:\n",
    "- `transaction_id` – уникальный идентификатор.\n",
    "- `customer_id` – ID клиента.\n",
    "- `amount` – сумма транзакции.\n",
    "- `transaction_date` – дата транзакции.\n",
    "- `category` – категория транзакции (Grocery, Electronics, Entertainment и т. д.).\n",
    "\n",
    "### **Задание**\n",
    "1. Найдите топ-5 клиентов с наибольшими расходами за последний месяц (0.5 б).\n",
    "2. Определите тренд покупок по категориям: сгруппируйте данные по неделям и посчитайте общую сумму покупок (1 б).\n",
    "3. Используйте оконную функцию, чтобы посчитать скользящее среднее трат клиента за последние 3 транзакции (1.5 б).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n9TwIs2c4gFT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: faker in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (36.1.1)\n",
      "Requirement already satisfied: tzdata in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from faker) (2023.3)\n",
      "\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HNBFLPaGMeO2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/05 22:53:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/05 22:54:13 WARN TaskSetManager: Stage 0 contains a task of very large size (6456 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/05 22:54:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/03/05 22:54:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:54:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "25/03/05 22:54:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:54:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, expr\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BankTransactions\").getOrCreate()\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_rows = 1000000\n",
    "customers = [fake.uuid4() for _ in range(5000)]\n",
    "categories = [\"Grocery\", \"Electronics\", \"Entertainment\", \"Clothing\", \"Travel\"]\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        i,\n",
    "        random.choice(customers),\n",
    "        round(random.uniform(5, 1000), 2),\n",
    "        (datetime.today() - timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "        random.choice(categories)\n",
    "    )\n",
    "    for i in range(num_rows)\n",
    "]\n",
    "\n",
    "columns = [\"transaction_id\", \"customer_id\", \"amount\", \"transaction_date\", \"category\"]\n",
    "transactions_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "transactions_df.write.mode(\"overwrite\").parquet(\"transactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 22:54:16 WARN TaskSetManager: Stage 1 contains a task of very large size (6456 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/05 22:54:20 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 1 (TID 10): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+----------------+-------------+\n",
      "|transaction_id|         customer_id|amount|transaction_date|     category|\n",
      "+--------------+--------------------+------+----------------+-------------+\n",
      "|             0|446d110b-0dd1-450...|589.11|      2024-12-19|  Electronics|\n",
      "|             1|cc4710c2-b6d3-4c6...|357.82|      2024-11-22|Entertainment|\n",
      "|             2|bff2537d-e073-428...|383.43|      2024-10-07|Entertainment|\n",
      "|             3|37ee4fa5-9a7d-4f4...|166.13|      2024-12-21|       Travel|\n",
      "|             4|d45226cf-dafe-48a...|367.14|      2024-08-02|       Travel|\n",
      "|             5|83e83834-7b11-491...|656.95|      2024-05-16|     Clothing|\n",
      "|             6|7986eafa-3212-4b3...|421.21|      2024-12-07|      Grocery|\n",
      "|             7|49980263-534c-4ea...|  58.2|      2024-12-14|Entertainment|\n",
      "|             8|3a4d9c45-df25-481...|355.69|      2024-09-05|Entertainment|\n",
      "|             9|0652a85e-059b-41a...| 103.2|      2024-12-28|       Travel|\n",
      "|            10|6c47cf5a-8a7a-4f8...| 91.04|      2024-05-07|       Travel|\n",
      "|            11|468292a9-da1b-4be...|199.95|      2024-10-24|      Grocery|\n",
      "|            12|5b6a6a93-abac-434...|983.11|      2024-11-10|     Clothing|\n",
      "|            13|9ee87905-4ecb-48e...|806.75|      2025-01-13|  Electronics|\n",
      "|            14|044d991e-0440-455...|607.49|      2025-02-09|      Grocery|\n",
      "|            15|d63648f5-28bc-401...|695.43|      2024-12-12|      Grocery|\n",
      "|            16|1edb2da5-2fa1-495...|897.61|      2024-05-07|       Travel|\n",
      "|            17|4ddaf20d-a177-420...|198.08|      2025-02-16|     Clothing|\n",
      "|            18|3d3a42b9-67e4-4e6...| 28.92|      2024-11-24|      Grocery|\n",
      "|            19|f13a0bc7-0d33-4ab...| 766.8|      2024-09-26|Entertainment|\n",
      "+--------------+--------------------+------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|         customer_id|       total_spent|\n",
      "+--------------------+------------------+\n",
      "|4f18cfd8-9b35-47c...|          19462.24|\n",
      "|6285c9b7-a027-499...|          17477.82|\n",
      "|7f1137bd-ab0d-4b5...|           16684.5|\n",
      "|676331ee-cbc9-45f...|          16336.27|\n",
      "|8d8eb288-8ab3-43f...|16265.130000000001|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "transactions_df = spark.read.parquet(\"transactions.parquet\").withColumn(\"transaction_date\", F.to_date(\"transaction_date\"))\n",
    "last_month = (datetime.today() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
    "last_month_transactions = transactions_df.filter(F.col(\"transaction_date\") >= last_month)\n",
    "result = last_month_transactions.groupBy(\"customer_id\").agg(F.sum(\"amount\").alias(\"total_spent\")).orderBy(F.desc(\"total_spent\")).limit(5)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================>                                  (4 + 6) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------------+\n",
      "|week|     category|       total_spent|\n",
      "+----+-------------+------------------+\n",
      "|   1|     Clothing|1926350.9100000006|\n",
      "|   1|  Electronics|1904905.3199999998|\n",
      "|   1|Entertainment|1988606.5299999998|\n",
      "|   1|      Grocery|1950249.6099999999|\n",
      "|   1|       Travel|1945807.6000000015|\n",
      "|   2|     Clothing|1935688.1600000001|\n",
      "|   2|  Electronics|1946308.7299999995|\n",
      "|   2|Entertainment|1904798.5200000003|\n",
      "|   2|      Grocery|1887951.3399999996|\n",
      "|   2|       Travel|1949722.3000000003|\n",
      "|   3|     Clothing|        1906964.76|\n",
      "|   3|  Electronics|1973861.2800000007|\n",
      "|   3|Entertainment|        1949380.57|\n",
      "|   3|      Grocery|1872115.6800000002|\n",
      "|   3|       Travel|        1886989.94|\n",
      "|   4|     Clothing|1977759.6900000002|\n",
      "|   4|  Electronics|        1912453.57|\n",
      "|   4|Entertainment|1933568.5800000003|\n",
      "|   4|      Grocery|         1919330.9|\n",
      "|   4|       Travel|1843711.4000000004|\n",
      "+----+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_with_week = transactions_df.withColumn(\"week\", F.weekofyear(\"transaction_date\"))\n",
    "result_2 = transactions_with_week.groupBy(\"week\", \"category\").agg(F.sum(\"amount\").alias(\"total_spent\"))\n",
    "result_2 = result_2.orderBy(\"week\", \"category\")\n",
    "result_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====>                                                    (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+----------------+-------------+------------------+\n",
      "|transaction_id|         customer_id|amount|transaction_date|     category|        moving_avg|\n",
      "+--------------+--------------------+------+----------------+-------------+------------------+\n",
      "|        176175|00169abe-c977-467...|417.47|      2024-03-07|  Electronics|            417.47|\n",
      "|        896155|00169abe-c977-467...|173.72|      2024-03-10|Entertainment|           295.595|\n",
      "|        987507|00169abe-c977-467...|883.47|      2024-03-13|      Grocery|491.55333333333334|\n",
      "|        742248|00169abe-c977-467...|300.86|      2024-03-15|Entertainment| 452.6833333333334|\n",
      "|        428788|00169abe-c977-467...| 354.0|      2024-03-16|Entertainment| 512.7766666666666|\n",
      "|        800845|00169abe-c977-467...|983.46|      2024-03-16|       Travel| 546.1066666666667|\n",
      "|        631871|00169abe-c977-467...|487.27|      2024-03-18|  Electronics| 608.2433333333333|\n",
      "|        316706|00169abe-c977-467...|463.14|      2024-03-19|Entertainment| 644.6233333333333|\n",
      "|        118290|00169abe-c977-467...|866.28|      2024-03-22|Entertainment| 605.5633333333334|\n",
      "|        497234|00169abe-c977-467...|955.41|      2024-03-22|      Grocery|            761.61|\n",
      "|        781760|00169abe-c977-467...|369.46|      2024-03-24|  Electronics| 730.3833333333333|\n",
      "|        280739|00169abe-c977-467...|589.26|      2024-03-27|      Grocery| 638.0433333333333|\n",
      "|        443105|00169abe-c977-467...| 998.3|      2024-03-27|      Grocery|            652.34|\n",
      "|        157801|00169abe-c977-467...|210.06|      2024-03-31|       Travel| 599.2066666666666|\n",
      "|        889257|00169abe-c977-467...|811.75|      2024-04-01|     Clothing|            673.37|\n",
      "|         20728|00169abe-c977-467...|813.45|      2024-04-04|      Grocery| 611.7533333333333|\n",
      "|        342798|00169abe-c977-467...|213.45|      2024-04-04|       Travel| 612.8833333333333|\n",
      "|        204323|00169abe-c977-467...|982.86|      2024-04-05|      Grocery| 669.9200000000001|\n",
      "|        735769|00169abe-c977-467...|486.88|      2024-04-05|  Electronics| 561.0633333333334|\n",
      "|        732946|00169abe-c977-467...|949.46|      2024-04-06|      Grocery|             806.4|\n",
      "+--------------+--------------------+------+----------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\").rowsBetween(-2, 0)\n",
    "transactions_with_moving_avg = transactions_df.withColumn(\"moving_avg\", F.avg(\"amount\").over(window_spec))\n",
    "transactions_with_moving_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljg67pwcNPnz"
   },
   "source": [
    "## **Задача 2: Анализ заказов на Amazon (1.5б)**\n",
    "\n",
    "Есть два набора данных:\n",
    "1. `orders` – информация о заказах на Amazon.\n",
    "2. `products` – информация о товарах.\n",
    "\n",
    "### **Задание**\n",
    "1. Найти 5 самых популярных товаров (по количеству заказов) (0.5 б).\n",
    "2. Определить для каждого пользователя средний чек и медианный чек (0.5 б).\n",
    "3. Найти товары, у которых средняя оценка выше 4.5, но они заказывались реже 100 раз (0.5 б)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YNCug0R5NTe2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 22:54:36 WARN TaskSetManager: Stage 13 contains a task of very large size (1304 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/05 22:54:36 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/03/05 22:54:36 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:54:36 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "25/03/05 22:54:37 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:54:37 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_orders = 500000\n",
    "num_products = 10000\n",
    "\n",
    "products = [(i, fake.word(), round(random.uniform(5, 500), 2), round(random.uniform(1, 5), 2))\n",
    "            for i in range(num_products)]\n",
    "orders = [\n",
    "    (i, random.randint(1, 50000), random.randint(0, num_products - 1), random.randint(1, 5),\n",
    "     (datetime.today() - timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"))\n",
    "    for i in range(num_orders)\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(products, [\"product_id\", \"product_name\", \"price\", \"rating\"])\n",
    "orders_df = spark.createDataFrame(orders, [\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"order_date\"])\n",
    "\n",
    "products_df.write.mode(\"overwrite\").parquet(\"products.parquet\")\n",
    "orders_df.write.mode(\"overwrite\").parquet(\"orders.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+------+------+\n",
      "|product_id|order_count|product_name| price|rating|\n",
      "+----------+-----------+------------+------+------+\n",
      "|      9658|         82|   professor| 14.07|  1.73|\n",
      "|      7044|         76|    industry| 91.57|  4.58|\n",
      "|      7152|         77|        fish|  97.3|  4.51|\n",
      "|      6384|         76|    position| 227.3|  1.37|\n",
      "|      5307|         80|    indicate|155.98|   2.7|\n",
      "+----------+-----------+------------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "products_df = spark.read.parquet(\"products.parquet\")\n",
    "orders_df = spark.read.parquet(\"orders.parquet\")\n",
    "product_popularity = orders_df.groupBy(\"product_id\").agg(F.count(\"order_id\").alias(\"order_count\"))\n",
    "result = product_popularity.orderBy(F.desc(\"order_count\")).limit(5).join(products_df, \"product_id\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+\n",
      "|customer_id|         avg_check|      median_check|\n",
      "+-----------+------------------+------------------+\n",
      "|          7|           719.962|            705.28|\n",
      "|         19| 769.5733333333333|            531.02|\n",
      "|         22| 789.8585714285715|414.06000000000006|\n",
      "|         26| 960.5871428571428|            517.53|\n",
      "|         29| 693.1858333333333|            348.22|\n",
      "|         34| 897.7600000000001|            989.44|\n",
      "|         50|1061.1163636363635|           1146.08|\n",
      "|         54| 982.5925000000001|            980.84|\n",
      "|         65|          1165.625| 781.0500000000001|\n",
      "|         77| 410.3999999999999|            330.82|\n",
      "|         94|          732.6025| 655.6500000000001|\n",
      "|        110|            959.46|            877.48|\n",
      "|        112| 659.3166666666666|            493.68|\n",
      "|        113|1097.0354545454547|           1202.15|\n",
      "|        126| 824.5574999999999|            716.54|\n",
      "|        130|         719.97625|            557.52|\n",
      "|        149| 639.3233333333333|            306.15|\n",
      "|        155| 880.1945454545454| 896.6500000000001|\n",
      "|        167| 210.9333333333333|             97.84|\n",
      "|        184| 723.7814285714285|            505.38|\n",
      "+-----------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = orders_df.join(products_df.select(\"product_id\", \"price\"), \"product_id\")\n",
    "orders_with_total = data.withColumn(\"total\", F.col(\"price\") * F.col(\"quantity\"))\n",
    "result = orders_with_total.groupBy(\"customer_id\").agg(\n",
    "    F.avg(\"total\").alias(\"avg_check\"), F.expr(\"percentile_approx(total, 0.5)\").alias(\"median_check\")\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+--------+----------+\n",
      "|order_id|customer_id|product_id|quantity|order_date|\n",
      "+--------+-----------+----------+--------+----------+\n",
      "|   49152|      37128|      7044|       5|2025-03-02|\n",
      "|   49153|       7175|      5824|       2|2024-09-13|\n",
      "|   49154|      44933|      3459|       2|2024-12-20|\n",
      "|   49155|      47281|      8256|       2|2024-08-30|\n",
      "|   49156|       3301|         3|       2|2024-05-25|\n",
      "|   49157|      17704|      2178|       1|2024-08-09|\n",
      "|   49158|      27697|      8627|       4|2024-09-21|\n",
      "|   49159|       5504|      5753|       1|2025-02-15|\n",
      "|   49160|      44773|       523|       3|2024-03-14|\n",
      "|   49161|      12451|      4743|       3|2024-04-16|\n",
      "|   49162|      13194|      3680|       5|2024-11-24|\n",
      "|   49163|      39606|      4538|       4|2024-03-17|\n",
      "|   49164|      10982|      8606|       2|2024-03-12|\n",
      "|   49165|      18132|      7574|       5|2024-10-09|\n",
      "|   49166|       1844|      3033|       1|2024-11-03|\n",
      "|   49167|      30963|      7195|       3|2024-06-05|\n",
      "|   49168|      24849|      9491|       5|2024-03-05|\n",
      "|   49169|       1565|      9556|       3|2024-07-27|\n",
      "|   49170|      17094|      7399|       3|2024-12-17|\n",
      "|   49171|      27340|      9164|       4|2024-06-24|\n",
      "+--------+-----------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------+------------+\n",
      "|product_id|       avg_rating|order_count|product_name|\n",
      "+----------+-----------------+-----------+------------+\n",
      "|      1677|             4.95|         62|       coach|\n",
      "|      4894|             4.53|         48|       cause|\n",
      "|      5409|4.660000000000001|         57|         add|\n",
      "|      8440|             4.76|         50|       claim|\n",
      "|      2040|             4.71|         51|       smile|\n",
      "|      3091|             4.69|         41|       truth|\n",
      "|      1806|             4.72|         56|       green|\n",
      "|      7436|             4.94|         43|         key|\n",
      "|      8209|             4.72|         54|        long|\n",
      "|      9179|             4.91|         55|     success|\n",
      "|      5241|4.960000000000001|         57|       state|\n",
      "|       558|             4.78|         55|        stop|\n",
      "|      6424|             4.52|         52|     subject|\n",
      "|      8092|4.610000000000001|         58|        food|\n",
      "|      6675|4.909999999999999|         37|          up|\n",
      "|      1642|4.960000000000001|         54|    approach|\n",
      "|      9712|             4.61|         44|   necessary|\n",
      "|      2909|4.579999999999999|         57|       space|\n",
      "|      2371|             4.58|         45|     teacher|\n",
      "|      6129|             4.94|         51|   challenge|\n",
      "+----------+-----------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = orders_df.join(products_df, \"product_id\").groupBy(\"product_id\").agg(\n",
    "    F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "    F.count(\"order_id\").alias(\"order_count\")\n",
    ")\n",
    "\n",
    "data = data.join(products_df.select(\"product_id\", \"product_name\"), \"product_id\")\n",
    "result = data.filter((F.col(\"avg_rating\") > 4.5) & (F.col(\"order_count\") < 100))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9MtsRiANZcA"
   },
   "source": [
    "## **Задача 3: Анализ задержек авиарейсов (3б)**\n",
    "\n",
    "У нас есть данные о 1 миллионе авиарейсов.\n",
    "\n",
    "### **Задание**\n",
    "1. Найти 10 самых задерживающихся авиакомпаний (0.5 б).\n",
    "2. Определить влияние времени суток на задержку (утром, днём, вечером) (1 б).\n",
    "3. Вычислить скользящее среднее задержек за последние 5 дней для каждого маршрута (1.5 б)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VV-Jnq9zNcAx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 22:55:07 WARN TaskSetManager: Stage 32 contains a task of very large size (3806 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/05 22:55:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/03/05 22:55:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:55:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "25/03/05 22:55:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:55:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_flights = 1000000\n",
    "airlines = [\"Delta\", \"United\", \"American Airlines\", \"Southwest\", \"Alaska Airlines\"]\n",
    "airports = [\"JFK\", \"LAX\", \"ORD\", \"DFW\", \"SFO\", \"ATL\", \"MIA\"]\n",
    "\n",
    "flights = [\n",
    "    (i, random.choice(airlines), random.choice(airports), random.choice(airports),\n",
    "     random.randint(-10, 180), random.randint(-15, 200),\n",
    "     (datetime.today() - timedelta(days=random.randint(0, 365), hours=random.randint(0, 23), minutes=random.randint(0, 59))).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    for i in range(num_flights)\n",
    "]\n",
    "\n",
    "flights_df = spark.createDataFrame(flights, [\"flight_id\", \"airline\", \"origin\", \"destination\", \"departure_delay\", \"arrival_delay\", \"flight_date\"])\n",
    "flights_df.write.mode(\"overwrite\").parquet(\"flights.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|          airline|        avg_delay|\n",
      "+-----------------+-----------------+\n",
      "|  Alaska Airlines|92.89529542452264|\n",
      "|American Airlines| 92.5935237981213|\n",
      "|           United|92.57446446956747|\n",
      "|            Delta|92.39867658394482|\n",
      "|        Southwest|92.39472143404045|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "flights_df = spark.read.parquet(\"flights.parquet\")\n",
    "result = flights_df.groupBy(\"airline\").agg(F.avg(\"arrival_delay\").alias(\"avg_delay\")).orderBy(F.desc(\"avg_delay\")).limit(10)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------+-----------+---------------+-------------+-------------------+\n",
      "|flight_id|          airline|origin|destination|departure_delay|arrival_delay|        flight_date|\n",
      "+---------+-----------------+------+-----------+---------------+-------------+-------------------+\n",
      "|   699392|  Alaska Airlines|   MIA|        LAX|            118|          -14|2025-02-08 10:04:45|\n",
      "|   699393|American Airlines|   ORD|        ORD|             83|           99|2024-11-29 11:47:45|\n",
      "|   699394|American Airlines|   SFO|        DFW|             78|          138|2024-07-15 08:46:45|\n",
      "|   699395|           United|   ATL|        ORD|             54|           99|2024-03-24 10:58:45|\n",
      "|   699396|           United|   JFK|        ATL|             14|          182|2024-08-24 04:56:45|\n",
      "|   699397|           United|   JFK|        LAX|             61|           75|2024-10-22 16:26:45|\n",
      "|   699398|  Alaska Airlines|   ORD|        DFW|            105|           44|2024-10-30 09:00:45|\n",
      "|   699399|  Alaska Airlines|   LAX|        MIA|            114|          170|2025-01-30 10:03:45|\n",
      "|   699400|            Delta|   ORD|        SFO|            121|           73|2025-01-22 03:13:45|\n",
      "|   699401|        Southwest|   LAX|        MIA|             70|          152|2024-08-29 08:21:45|\n",
      "|   699402|        Southwest|   DFW|        SFO|             47|          -11|2024-06-27 08:21:45|\n",
      "|   699403|        Southwest|   JFK|        ORD|             74|           32|2025-02-24 00:19:45|\n",
      "|   699404|American Airlines|   SFO|        DFW|            171|          159|2025-02-16 03:21:45|\n",
      "|   699405|  Alaska Airlines|   ATL|        ORD|            133|          120|2025-01-27 10:05:45|\n",
      "|   699406|           United|   JFK|        MIA|            148|           79|2025-02-08 09:27:45|\n",
      "|   699407|        Southwest|   SFO|        LAX|            109|          184|2024-12-05 06:58:45|\n",
      "|   699408|           United|   ATL|        JFK|             39|           -4|2024-10-28 02:07:45|\n",
      "|   699409|           United|   LAX|        SFO|            107|           -3|2024-11-28 06:58:45|\n",
      "|   699410|        Southwest|   JFK|        ATL|             14|          146|2024-09-09 20:21:45|\n",
      "|   699411|           United|   JFK|        MIA|             81|          170|2024-12-25 05:48:45|\n",
      "+---------+-----------------+------+-----------+---------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|time_of_day|        avg_delay|\n",
      "+-----------+-----------------+\n",
      "|  afternoon|92.54476631471007|\n",
      "|    morning|92.54736959696145|\n",
      "|    evening| 92.5966896270177|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "data = flights_df.withColumn(\n",
    "    \"time_of_day\",\n",
    "    when((F.hour(\"flight_date\") >= 6) & (F.hour(\"flight_date\") < 12), \"morning\")\n",
    "    .when((F.hour(\"flight_date\") >= 12) & (F.hour(\"flight_date\") < 18), \"afternoon\")\n",
    "    .otherwise(\"evening\")\n",
    ")\n",
    "result = data.groupBy(\"time_of_day\").agg(F.avg(\"arrival_delay\").alias(\"avg_delay\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не имеет значение время суток, просто погрешность, может утром люди чуть менее уставшие и лучше работают:)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------+-----------+---------------+-------------+-------------------+-------+--------------+----------------+\n",
      "|flight_id|          airline|origin|destination|departure_delay|arrival_delay|        flight_date|  route|departure_date|moving_avg_delay|\n",
      "+---------+-----------------+------+-----------+---------------+-------------+-------------------+-------+--------------+----------------+\n",
      "|   776958|        Southwest|   MIA|        DFW|              0|            0|2024-03-04 23:50:46|MIA-DFW|    2024-03-04|             0.0|\n",
      "|   893797|        Southwest|   MIA|        DFW|             75|           32|2024-03-04 23:41:46|MIA-DFW|    2024-03-04|            16.0|\n",
      "|   275235|            Delta|   MIA|        DFW|             37|           31|2024-03-04 23:26:43|MIA-DFW|    2024-03-04|            21.0|\n",
      "|   913959|  Alaska Airlines|   MIA|        DFW|             97|          199|2024-03-04 23:53:46|MIA-DFW|    2024-03-04|            65.5|\n",
      "|    73764|American Airlines|   MIA|        DFW|             90|           98|2024-03-04 23:52:41|MIA-DFW|    2024-03-04|            72.0|\n",
      "|    85318|            Delta|   MIA|        DFW|            128|           -1|2024-03-04 23:05:41|MIA-DFW|    2024-03-04|            71.8|\n",
      "|   716801|        Southwest|   MIA|        DFW|            126|           60|2024-03-05 09:59:45|MIA-DFW|    2024-03-05|            77.4|\n",
      "|   727218|  Alaska Airlines|   MIA|        DFW|            130|           90|2024-03-05 12:17:45|MIA-DFW|    2024-03-05|            89.2|\n",
      "|   773800|           United|   MIA|        DFW|            151|          190|2024-03-05 10:35:46|MIA-DFW|    2024-03-05|            87.4|\n",
      "|   403157|           United|   MIA|        DFW|             11|            6|2024-03-05 08:05:43|MIA-DFW|    2024-03-05|            69.0|\n",
      "|   424576|            Delta|   MIA|        DFW|             64|           97|2024-03-05 20:56:43|MIA-DFW|    2024-03-05|            88.6|\n",
      "|   438679|American Airlines|   MIA|        DFW|             25|           26|2024-03-05 06:46:44|MIA-DFW|    2024-03-05|            81.8|\n",
      "|   442147|  Alaska Airlines|   MIA|        DFW|             18|           10|2024-03-05 19:39:44|MIA-DFW|    2024-03-05|            65.8|\n",
      "|   446601|        Southwest|   MIA|        DFW|            124|           92|2024-03-05 03:08:44|MIA-DFW|    2024-03-05|            46.2|\n",
      "|   453157|American Airlines|   MIA|        DFW|            158|           -5|2024-03-05 01:15:44|MIA-DFW|    2024-03-05|            44.0|\n",
      "|   471920|            Delta|   MIA|        DFW|            116|          -11|2024-03-05 15:25:44|MIA-DFW|    2024-03-05|            22.4|\n",
      "|   832460|American Airlines|   MIA|        DFW|             69|           26|2024-03-05 09:00:46|MIA-DFW|    2024-03-05|            22.4|\n",
      "|   854593|  Alaska Airlines|   MIA|        DFW|             65|          153|2024-03-05 23:13:46|MIA-DFW|    2024-03-05|            51.0|\n",
      "|   866267|            Delta|   MIA|        DFW|            123|          171|2024-03-05 05:44:46|MIA-DFW|    2024-03-05|            66.8|\n",
      "|   869434|  Alaska Airlines|   MIA|        DFW|             17|          194|2024-03-05 05:45:46|MIA-DFW|    2024-03-05|           106.6|\n",
      "+---------+-----------------+------+-----------+---------------+-------------+-------------------+-------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "flights_df = flights_df.withColumn(\"route\", F.concat(F.col(\"origin\"), F.lit(\"-\"), F.col(\"destination\")))\n",
    "flights_df = flights_df.withColumn(\"departure_date\", F.to_date(\"flight_date\"))\n",
    "window_spec = Window.partitionBy(\"route\").orderBy(\"departure_date\").rowsBetween(-4, 0)\n",
    "result = flights_df.withColumn(\"moving_avg_delay\", F.avg(\"arrival_delay\").over(window_spec))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В колонках есть отрицательные числа, не знаю нужно ли их убрать, но я считаю что с ними среднее лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqWLC2YSNx6-"
   },
   "source": [
    "## **Задача 4: Анализ поведения пользователей в мобильном приложении (1б)**\n",
    "\n",
    "У нас есть лог-файл с действиями пользователей в мобильном приложении. Каждая строка представляет событие пользователя.\n",
    "- `event_id` – ID события.\n",
    "- `user_id` – ID пользователя.\n",
    "- `event_type` – Тип события (`login`, `view_product`, `add_to_cart`, `purchase`).\n",
    "- `event_timestamp` – Время события.\n",
    "\n",
    "### **Задание**\n",
    "1. Определите время первой и последней активности каждого пользователя (0.5 б).\n",
    "2. Рассчитайте среднюю продолжительность сессии для каждого пользователя (0.5 б)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YVSz757NN0s6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 22:55:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/03/05 22:55:34 WARN TaskSetManager: Stage 44 contains a task of very large size (6740 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/05 22:55:34 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/03/05 22:55:34 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:55:34 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "25/03/05 22:55:35 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:55:35 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, expr\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UserBehaviorAnalysis\").getOrCreate()\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_rows = 1000000\n",
    "users = [fake.uuid4() for _ in range(50000)]\n",
    "event_types = [\"login\", \"view_product\", \"add_to_cart\", \"purchase\"]\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        i,\n",
    "        random.choice(users),\n",
    "        random.choice(event_types),\n",
    "        (datetime.today() - timedelta(days=random.randint(0, 30), hours=random.randint(0, 23), minutes=random.randint(0, 59))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    for i in range(num_rows)\n",
    "]\n",
    "\n",
    "columns = [\"event_id\", \"user_id\", \"event_type\", \"event_timestamp\"]\n",
    "events_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "events_df.write.mode(\"overwrite\").parquet(\"user_events.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+\n",
      "|             user_id|     first_activity|      last_activity|\n",
      "+--------------------+-------------------+-------------------+\n",
      "|1c0b29f9-d2fc-454...|2025-02-03 01:46:16|2025-03-04 20:53:12|\n",
      "|66a7d6e7-121e-48c...|2025-02-03 12:10:16|2025-03-03 07:34:13|\n",
      "|0eddc998-ce13-473...|2025-02-03 10:22:13|2025-03-05 19:11:13|\n",
      "|ca17a52f-ad52-4af...|2025-02-03 01:40:14|2025-03-05 14:17:15|\n",
      "|d6a31251-5a48-4bf...|2025-02-03 18:37:17|2025-03-05 01:09:12|\n",
      "|366ec8f2-5d6f-44b...|2025-02-03 18:32:16|2025-03-04 19:41:15|\n",
      "|246940a1-9dbe-49c...|2025-02-04 11:16:13|2025-03-04 02:24:13|\n",
      "|0795b3f7-49d7-483...|2025-02-03 07:16:17|2025-03-02 03:11:15|\n",
      "|499bdfd3-827e-4df...|2025-02-04 11:07:15|2025-03-05 11:23:13|\n",
      "|bb9e73db-edae-46e...|2025-02-06 09:43:12|2025-03-05 08:33:16|\n",
      "|0eebfa40-fed5-46a...|2025-02-05 15:29:17|2025-03-05 01:22:14|\n",
      "|94656142-1c62-447...|2025-02-03 07:28:12|2025-03-03 18:49:17|\n",
      "|f50c3740-3d2c-46a...|2025-02-04 16:01:15|2025-03-04 09:01:14|\n",
      "|01a40af2-fb64-470...|2025-02-03 03:21:14|2025-03-02 06:48:13|\n",
      "|75d86d9b-f95f-427...|2025-02-03 13:10:14|2025-03-03 02:16:14|\n",
      "|4e491298-c481-4b6...|2025-02-04 13:45:15|2025-03-04 03:38:15|\n",
      "|53d199ce-84c6-453...|2025-02-03 08:42:13|2025-03-05 05:32:15|\n",
      "|cd537e45-aa17-493...|2025-02-03 05:15:14|2025-03-04 08:26:15|\n",
      "|889d2eee-433b-467...|2025-02-03 00:49:15|2025-03-05 02:40:14|\n",
      "|a5cff79d-e2de-4e2...|2025-02-04 05:41:13|2025-03-03 00:08:12|\n",
      "+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "events_df = spark.read.parquet(\"user_events.parquet\").withColumn(\"event_timestamp\", F.to_timestamp(\"event_timestamp\"))\n",
    "result = events_df.groupBy(\"user_id\").agg(F.min(\"event_timestamp\").alias(\"first_activity\"),\n",
    "                                                 F.max(\"event_timestamp\").alias(\"last_activity\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+-------------------+\n",
      "|event_id|             user_id|  event_type|    event_timestamp|\n",
      "+--------+--------------------+------------+-------------------+\n",
      "|  699392|3e488210-b3ff-4db...|       login|2025-03-01 18:13:15|\n",
      "|  699393|89ae8a49-c018-416...|       login|2025-03-03 05:59:15|\n",
      "|  699394|0ca93e6d-e4e0-4ef...|    purchase|2025-02-19 14:23:15|\n",
      "|  699395|1f61028a-1efb-4f1...|       login|2025-02-28 07:33:15|\n",
      "|  699396|f3692a13-0579-48c...|    purchase|2025-02-17 08:25:15|\n",
      "|  699397|72862a2f-56eb-4cd...| add_to_cart|2025-02-03 19:54:15|\n",
      "|  699398|7f83ed0b-86ba-4da...|       login|2025-02-21 18:24:15|\n",
      "|  699399|772b363d-b24f-4e8...|    purchase|2025-03-02 14:22:15|\n",
      "|  699400|60a5127b-6f44-4d2...|    purchase|2025-02-19 14:08:15|\n",
      "|  699401|e3fa8593-0234-403...| add_to_cart|2025-02-05 03:46:15|\n",
      "|  699402|ae54a7a0-72d3-471...| add_to_cart|2025-02-25 00:46:15|\n",
      "|  699403|c298b3af-1dcd-49c...|       login|2025-02-13 07:21:15|\n",
      "|  699404|ecdb66d2-d801-4d1...|       login|2025-02-02 23:54:15|\n",
      "|  699405|ca7a4d29-ec65-457...|view_product|2025-02-26 17:57:15|\n",
      "|  699406|bc4b1387-2ac4-429...|view_product|2025-02-23 21:37:15|\n",
      "|  699407|823d2d07-e050-4be...|view_product|2025-02-13 14:37:15|\n",
      "|  699408|4fbb8c53-c867-495...|    purchase|2025-02-12 16:58:15|\n",
      "|  699409|fb0a133a-af43-466...|view_product|2025-02-23 10:24:15|\n",
      "|  699410|e7a1842a-1409-4de...|    purchase|2025-02-04 06:14:15|\n",
      "|  699411|7c3afd45-a608-482...|    purchase|2025-02-15 05:18:15|\n",
      "+--------+--------------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             user_id|avg_session_duration|\n",
      "+--------------------+--------------------+\n",
      "|0323786f-3d2c-4bc...|           229696.25|\n",
      "|0795b3f7-49d7-483...|            670667.6|\n",
      "|07e16e4d-0899-4ad...|           -252661.0|\n",
      "|0af6011a-be4f-478...|           -722938.0|\n",
      "|0c71c1d6-8c1b-4da...|            865559.0|\n",
      "|0eddc998-ce13-473...|           -905188.5|\n",
      "|0eebfa40-fed5-46a...|  -431930.6666666667|\n",
      "|10a7996c-bb4d-415...|           1057056.4|\n",
      "|13bfdf0b-c331-483...|   280499.3333333333|\n",
      "|1c0b29f9-d2fc-454...|            302960.5|\n",
      "|2094d18e-c043-447...|         -1741231.25|\n",
      "|235e813d-e624-467...|            810394.4|\n",
      "|279f2542-dc74-4ba...|           -310511.6|\n",
      "|2c09135d-4194-4a2...|            592378.0|\n",
      "|2c2647f5-bfa0-4d6...|          -1448489.0|\n",
      "|31672510-270c-459...|            146743.0|\n",
      "|32bb55f1-7840-4d7...|           -294600.2|\n",
      "|366ec8f2-5d6f-44b...|  164369.33333333334|\n",
      "|3936f642-4e91-4e1...|           -173460.5|\n",
      "|3aed6674-a9ad-4e0...| -20443.714285714286|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"event_timestamp\")\n",
    "login_events = events_df.filter(F.col(\"event_type\") == \"login\") \\\n",
    "    .withColumn(\"login_row\", F.row_number().over(window_spec)) \\\n",
    "    .select(\"user_id\", \"event_timestamp\", \"login_row\") \\\n",
    "    .withColumnRenamed(\"event_timestamp\", \"session_start\")\n",
    "purchase_events = events_df.filter(F.col(\"event_type\") == \"purchase\") \\\n",
    "    .withColumn(\"purchase_row\", F.row_number().over(window_spec)) \\\n",
    "    .select(\"user_id\", \"event_timestamp\", \"purchase_row\") \\\n",
    "    .withColumnRenamed(\"event_timestamp\", \"session_end\")\n",
    "sessions = login_events.join(purchase_events, (login_events.user_id == purchase_events.user_id) & (login_events.login_row == purchase_events.purchase_row)) \\\n",
    "    .select(login_events.user_id, \"session_start\", \"session_end\")\n",
    "sessions = sessions.withColumn(\"session_duration\", F.unix_timestamp(\"session_end\") - F.unix_timestamp(\"session_start\"))\n",
    "avg_session_duration = sessions.groupBy(\"user_id\").agg(F.avg(\"session_duration\").alias(\"avg_session_duration\"))\n",
    "avg_session_duration.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы занумеровали события входа и выхода и потом считали разницу с i заходом и i выходом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5ThWerbEJBt"
   },
   "source": [
    "## **Задача 5: Анализ трафика на сайте (1.5б)**\n",
    "\n",
    "Есть лог-файл с посещениями веб-сайта, в котором записано:\n",
    "- `session_id` – ID сессии.\n",
    "- `user_id` – ID пользователя.\n",
    "- `ip_address` – IP-адрес посетителя.\n",
    "- `page_url` – Посещённая страница.\n",
    "- `timestamp` – Время визита.\n",
    "\n",
    "### **Задание**\n",
    "1. Найдите пиковые часы активности (сгруппируйте по часу) (0.5 б).\n",
    "2. Определите ботов (пользователей, сделавших >100 запросов за сутки) (0.5 б).\n",
    "3. Определите топ-5 стран по количеству посещений (с помощью IP-адресов) (0.5 б).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "614f1Kzz4dhD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: geoip2 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (5.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.6.2 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from geoip2) (3.8.6)\n",
      "Requirement already satisfied: maxminddb<3.0.0,>=2.5.1 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from geoip2) (2.6.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from geoip2) (2.32.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (24.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (3.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.18.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->geoip2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->geoip2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->geoip2) (2024.12.14)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kseniashk/anaconda3/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.6.2->geoip2) (0.2.1)\n",
      "\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/kseniashk/anaconda3/lib/python3.11/site-packages/matplotlib-3.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geoip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ly6NSY-dPJ3f",
    "outputId": "06939a7a-7765-4908-a050-78dcc0518d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "100 56.1M  100 56.1M    0     0  2671k      0  0:00:21  0:00:21 --:--:-- 4059k 0:00:18  0:00:05 4215k\n"
     ]
    }
   ],
   "source": [
    "! curl -L -o \"GeoLite2-City.mmdb\" \"https://git.io/GeoLite2-City.mmdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "5JmbO2pBORQz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 22:56:15 WARN TaskSetManager: Stage 67 contains a task of very large size (2513 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/05 22:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/03/05 22:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "25/03/05 22:56:16 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/03/05 22:56:16 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import geoip2.database\n",
    "\n",
    "geo_reader = geoip2.database.Reader(\"GeoLite2-City.mmdb\")\n",
    "\n",
    "num_rows = 200000\n",
    "sessions = [fake.uuid4() for _ in range(20000)]\n",
    "users = [fake.uuid4() for _ in range(100000)]\n",
    "ip_addresses = [fake.ipv4() for _ in range(5000)]\n",
    "pages = [\"/home\", \"/products\", \"/checkout\", \"/contact\", \"/about\"]\n",
    "\n",
    "data = []\n",
    "for i in range(num_rows):\n",
    "    ip = random.choice(ip_addresses)\n",
    "    try:\n",
    "        country = geo_reader.city(ip).country.name\n",
    "    except:\n",
    "        country = \"Unknown\"\n",
    "\n",
    "    data.append((\n",
    "        random.choice(sessions),\n",
    "        random.choice(users),\n",
    "        ip,\n",
    "        random.choice(pages),\n",
    "        (datetime.today() - timedelta(hours=random.randint(0, 23), minutes=random.randint(0, 59))).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        country\n",
    "    ))\n",
    "\n",
    "columns = [\"session_id\", \"user_id\", \"ip_address\", \"page_url\", \"timestamp\", \"country\"]\n",
    "traffic_df = spark.createDataFrame(data, columns)\n",
    "traffic_df.write.mode(\"overwrite\").parquet(\"web_traffic.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|hour|visit_count|\n",
      "+----+-----------+\n",
      "|  10|       8562|\n",
      "|  13|       8479|\n",
      "|  16|       8460|\n",
      "|   6|       8456|\n",
      "|   9|       8417|\n",
      "|  19|       8396|\n",
      "|   7|       8382|\n",
      "|  20|       8375|\n",
      "|  18|       8372|\n",
      "|  12|       8361|\n",
      "|  17|       8353|\n",
      "|   0|       8342|\n",
      "|  14|       8339|\n",
      "|   3|       8326|\n",
      "|  23|       8315|\n",
      "|   2|       8290|\n",
      "|   5|       8285|\n",
      "|  21|       8281|\n",
      "|   4|       8272|\n",
      "|  11|       8254|\n",
      "+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "traffic_df = spark.read.parquet(\"web_traffic.parquet\")\n",
    "traffic_df = traffic_df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
    "traffic_df = traffic_df.withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "data = traffic_df.groupBy(\"hour\").agg(F.count(\"session_id\").alias(\"visit_count\"))\n",
    "result = data.orderBy(F.desc(\"visit_count\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------------+\n",
      "|user_id|date|request_count|\n",
      "+-------+----+-------------+\n",
      "+-------+----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "traffic_df = traffic_df.withColumn(\"date\", F.to_date(\"timestamp\"))\n",
    "data = traffic_df.groupBy(\"user_id\", \"date\").agg(F.count(\"session_id\").alias(\"request_count\"))\n",
    "result = data.filter(F.col(\"request_count\") > 100)\n",
    "result.show() # нету("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|      country|visit_count|\n",
      "+-------------+-----------+\n",
      "|United States|      83161|\n",
      "|        China|      18017|\n",
      "|        Japan|       9516|\n",
      "|  South Korea|       7368|\n",
      "|      Germany|       7305|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = traffic_df.groupBy(\"country\").agg(F.count(\"session_id\").alias(\"visit_count\"))\n",
    "result = data.orderBy(F.desc(\"visit_count\")).limit(5)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WjkTH8QMMZL9",
    "Ljg67pwcNPnz",
    "qqWLC2YSNx6-",
    "E5ThWerbEJBt"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
